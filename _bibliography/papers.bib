---
---

@string{aps = {American Physical Society,}}


@inproceedings{lee2025accuquant,
  bibtex_show={true},
  title={AccuQuant: Simulating Multiple Denoising Steps for Quantizing Diffusion Models},
  author={Lee*, Seunghoon and Choi*, Jeongwoo and Son, Byunggwan and Moon, Jaehyeon and Jeon, Jeimin and Ham, Bumsub},
  booktitle={Conference on Neural Information Processing Systems (NeurIPS)},
  year={2025},
  html={https://cvlab.yonsei.ac.kr/projects/AccuQuant/},
  selected={true},
  preview={accuquant_overview.png},
  abbr={NeurIPS 2025 (poster)},
  pdf={https://arxiv.org/abs/2510.20348},
  abstract={We present in this paper a novel post-training quantization (PTQ) method, dubbed AccuQuant, for diffusion models. We show analytically and empirically that quantization errors for diffusion models are accumulated over denoising steps in a sampling process. To alleviate the error accumulation problem, AccuQuant mini5 mizes the discrepancies between outputs of a full-precision diffusion model and its quantized version within a couple of denoising steps. That is, it simulates multiple denoising steps of a diffusion sampling process explicitly for quantization, account8 ing the accumulated errors over multiple denoising steps, which is in contrast to previous approaches to imitating a training process of diffusion models, namely, minimizing the discrepancies independently for each step. We also present an efficient implementation technique for AccuQuant, together with a novel objective, which reduces a memory complexity significantly from O(n) to O(1), where n is the number of denoising steps. We demonstrate the efficacy and efficiency of AccuQuant across various tasks and diffusion models on standard benchmarks.}
}

@inproceedings{son2026relational,
  title={Relational Feature Caching for Accelerating Diffusion Transformers},
  author={Son*, Byunggwan and Jeon*, Jeimin and Choi*, Jeongwoo and Ham, Bumsub},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026},
  preview={},
  abbr={ICLR 2026 (under review)},
}